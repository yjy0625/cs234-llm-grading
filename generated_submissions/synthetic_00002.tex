%%%%% Start of 1(a) %%%%%
Yes. For example with \(H=5\) starting at \(s=3\), an optimal policy can sell three times to get to \(s=0\) and collect \(+3\), then buy once (to \(s=1\), reward \(0\)), then sell once more (back to \(s=0\), reward \(+1\)) for a total of \(+4\). You can’t do better in 5 steps because reaching \(s=10\) is impossible from \(s=3\) within 5 steps, and at most one unit of reward can be earned per step via selling.
%%%%% End of 1(a) %%%%%

%%%%% Start of 1(b) %%%%%
No. For any \(\gamma \in [0,1)\), the \(+100\) terminal reward for fully stocking will eventually dominate, so the optimal policy will always try to buy up to \(s=10\) rather than keep selling and avoid ever reaching full stock.
%%%%% End of 1(b) %%%%%

%%%%% Start of 1(c) %%%%%
No. A discounted infinite-horizon objective can’t exactly match any finite-horizon objective here, because the finite-horizon problem has a hard cutoff after \(H\) steps while the discounted problem still assigns (possibly small) value to rewards arbitrarily far in the future, which changes the tradeoff between selling now versus buying toward the \(+100\).
%%%%% End of 1(c) %%%%%

%%%%% Start of 1(d) %%%%%
Yes. For any finite \(H\), you can pick \(\gamma\) small enough that rewards after step \(H\) are effectively negligible, making the discounted optimal policy match the finite-horizon optimal behavior from the start state.
%%%%% End of 1(d) %%%%%

%%%%% Start of 2(a) %%%%%
If the red car merges, the human-driven cars on the highway have to brake/leave gaps to accommodate it, which lowers many cars’ speeds and reduces the average velocity. By instead parking and never entering, the red car’s low speed only affects one vehicle (itself) while the grey cars can keep going near full speed, so the mean velocity over all cars is higher. Thus, under the proxy objective, “don’t merge” is optimal even though it’s undesirable.
%%%%% End of 2(a) %%%%%

%%%%% Start of 2(b) %%%%%
Instead of mean velocity, reward could emphasize fairness or bottlenecks, e.g., focusing on the slowest/most-delayed vehicle so the policy is encouraged to merge smoothly rather than sacrifice one car entirely. Another idea is to add a penalty for being stopped/off-highway for too long, so the agent can’t maximize reward by opting out of traffic. You could also include a term that rewards successful progress toward entering and maintaining highway driving (e.g., positive reward for time spent on-route) while still discouraging creating large slowdowns for others.
%%%%% End of 2(b) %%%%%

%%%%% Start of 3(a) %%%%%
For any state \(s\),
\[
(B^\pi V)(s)-(B^\pi V')(s)=\gamma \sum_{s'} p(s'|s,\pi(s))\big(V(s')-V'(s')\big).
\]
Taking absolute values and using that probabilities sum to 1,
\[
\left|(B^\pi V)(s)-(B^\pi V')(s)\right|
\le \gamma \sum_{s'} p(s'|s,\pi(s))\,|V(s')-V'(s')|
\le \gamma \|V-V'\|.
\]
Then taking max over \(s\) gives \(\|B^\pi V-B^\pi V'\|\le \gamma\|V-V'\|\). (I’m implicitly using the sup norm and the fact the expectation can’t exceed the max; I didn’t separately write the “other direction” but it’s the same since we took absolute values.)
%%%%% End of 3(a) %%%%%

%%%%% Start of 3(b) %%%%%
Suppose \(V\) and \(V'\) are both fixed points of \(B^\pi\), so \(V=B^\pi V\) and \(V'=B^\pi V'\). Then
\[
\|V-V'\|=\|B^\pi V-B^\pi V'\|\le \gamma \|V-V'\|.
\]
Since \(\gamma<1\), this forces \(\|V-V'\|=0\), so \(V=V'\). Therefore the fixed point is unique. (This uses the contraction idea from part (a).)
%%%%% End of 3(b) %%%%%

%%%%% Start of 3(c) %%%%%
Assume \(V(s)\le V'(s)\) for all \(s\). Then for any \(s\),
\[
(B^\pi V)(s)= r(s,\pi(s))+\gamma \sum_{s'} p(s'|s,\pi(s))V(s')
\le r(s,\pi(s))+\gamma \sum_{s'} p(s'|s,\pi(s))V'(s')
=(B^\pi V')(s),
\]
since \(p(\cdot|s,\pi(s))\) is a probability distribution and preserves inequalities under expectation. Hence \(B^\pi\) is monotone.
%%%%% End of 3(c) %%%%%

%%%%% Start of 3(d) %%%%%
\(\|BV-V\|=0\) iff \(BV-V\) is the zero vector, i.e. iff \(BV=V\). So this happens exactly when \(V\) is a fixed point of the Bellman optimality operator \(B\). In that case \(V\) must equal the optimal value function \(V^*\) (the fixed point of \(B\)).
%%%%% End of 3(d) %%%%%

%%%%% Start of 3(e) %%%%%
First bound (policy evaluation residual). Since \(V^\pi\) is the fixed point of \(B^\pi\), \(V^\pi=B^\pi V^\pi\). Then
\[
\|V-V^\pi\|=\|V-B^\pi V^\pi\|
\le \|V-B^\pi V\|+\|B^\pi V-B^\pi V^\pi\|.
\]
By contraction of \(B^\pi\) (part (a)),
\[
\|B^\pi V-B^\pi V^\pi\|\le \gamma \|V-V^\pi\|.
\]
So
\[
\|V-V^\pi\|\le \|V-B^\pi V\|+\gamma\|V-V^\pi\|
\quad\Rightarrow\quad
(1-\gamma)\|V-V^\pi\|\le \|V-B^\pi V\|.
\]
Thus
\[
\boxed{\ \|V-V^\pi\|\le \frac{\|V-B^\pi V\|}{1-\gamma}\ }.
\]

Second bound (optimality residual). Since \(V^*=BV^*\),
\[
\|V-V^*\|\le \|V-BV\|+\|BV-BV^*\|
\le \|V-BV\|+\gamma\|V-V^*\|,
\]
using that \(B\) is also a \(\gamma\)-contraction in \(\|\cdot\|_\infty\). Rearranging gives
\[
\boxed{\ \|V-V^*\|\le \frac{\|V-BV\|}{1-\gamma}\ }.
\]
%%%%% End of 3(e) %%%%%

%%%%% Start of 3(f) %%%%%
Let \(\pi\) be greedy w.r.t. \(V\). Then by definition of greediness, for every \(s\),
\[
(BV)(s)=\max_a \Big[r(s,a)+\gamma\sum_{s'}p(s'|s,a)V(s')\Big]
= r(s,\pi(s))+\gamma\sum_{s'}p(s'|s,\pi(s))V(s')
=(B^\pi V)(s),
\]
so \(BV=B^\pi V\). Let \(\varepsilon:=\|BV-V\|=\|B^\pi V-V\|\).

Apply part (e) twice:
\[
\|V-V^\pi\|\le \frac{\|V-B^\pi V\|}{1-\gamma}=\frac{\varepsilon}{1-\gamma},
\qquad
\|V-V^*\|\le \frac{\|V-BV\|}{1-\gamma}=\frac{\varepsilon}{1-\gamma}.
\]
Then by triangle inequality,
\[
\|V^*-V^\pi\|\le \|V^*-V\|+\|V-V^\pi\|\le \frac{2\varepsilon}{1-\gamma}.
\]
Also \(V^\pi\le V^*\) componentwise, so this implies the one-sided performance guarantee:
\[
V^\pi(s)\ge V^*(s)-\frac{2\varepsilon}{1-\gamma}\quad \forall s.
\]
%%%%% End of 3(f) %%%%%

%%%%% Start of 3(g) %%%%%
One application is in robotics navigation: if you compute an approximate value function \(V\) from data and then act greedily w.r.t. \(V\), the bound in (f) tells you the greedy policy’s return is close to optimal whenever the Bellman residual \(\|BV-V\|\) is small, so you can trust the learned controller.
%%%%% End of 3(g) %%%%%

%%%%% Start of 3(h) %%%%%
Yes: if two value functions have the same Bellman error \(\|BV-V\|\), then they should be equally good approximations, so the greedy policies induced by them should have the same performance (up to the same bound), meaning they are basically comparable.
%%%%% End of 3(h) %%%%%

%%%%% Start of 3(i) %%%%%
Assume \(V^*\le V\) componentwise, and let \(\pi\) be greedy w.r.t. \(V\), so \(BV=B^\pi V\). Let \(\varepsilon=\|BV-V\|\).

From part (e),
\[
\|V-V^\pi\|\le \frac{\|V-B^\pi V\|}{1-\gamma}=\frac{\varepsilon}{1-\gamma}.
\]
Now use the ordering \(V^\pi\le V^*\le V\). Then for each state \(s\),
\[
0\le V^*(s)-V^\pi(s)\le V(s)-V^\pi(s)\le \|V-V^\pi\|\le \frac{\varepsilon}{1-\gamma}.
\]
So we get the tighter one-sided bound
\[
V^\pi(s)\ge V^*(s)-\frac{\varepsilon}{1-\gamma}\quad \forall s.
\]
(Here I used componentwise inequalities plus the sup-norm bound.)
%%%%% End of 3(i) %%%%%
